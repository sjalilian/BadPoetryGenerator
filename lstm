import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import json
import os

# Read the poems from the provided JSON file
def read_poems_from_json(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        poems_data = json.load(f)

    all_poems = ""
    for poem in poems_data:
        all_poems += poem['text'].replace("<START>", "").replace("<END>", "") + "\n\n"
    return all_poems

# Prepare the input text sequences for the LSTM model
def prepare_sequences(text, max_sequence_len):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts([text])

    sequences = []
    for line in text.split('\n'):
        token_list = tokenizer.texts_to_sequences([line])[0]
        for i in range(1, len(token_list)):
            n_gram_sequence = token_list[:i+1]
            sequences.append(n_gram_sequence)

    max_sequence_len = max([len(seq) for seq in sequences])
    sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='pre')

    X, y = sequences[:,:-1], sequences[:,-1]
    y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index) + 1)

    return X, y, tokenizer, max_sequence_len

# Build the LSTM model
def build_model(vocab_size, max_sequence_len):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, 100, input_length=max_sequence_len - 1),
        tf.keras.layers.LSTM(150),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(vocab_size, activation='softmax')
    ])

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Generate text using the trained model
def generate_text(seed_text, next_words, model, max_sequence_len, tokenizer):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')
        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted:
                output_word = word
                break
        seed_text += " " + output_word
    return seed_text

# Load the dataset and prepare sequences
file_path = '/content/cleaned_poems.json'
poems_text = read_poems_from_json(file_path)

X, y, tokenizer, max_sequence_len = prepare_sequences(poems_text, max_sequence_len=200)

# Build and train the model
model = build_model(vocab_size=len(tokenizer.word_index) + 1, max_sequence_len=max_sequence_len)
model.fit(X, y, epochs=100, verbose=2)

# Generate new text
seed_text = "The sun"
next_words = 20
generated_text = generate_text(seed_text, next_words, model, max_sequence_len, tokenizer)
print(generated_text)
